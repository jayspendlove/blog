{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing flows for probability distribution reconstruction\n",
    "\n",
    "Normalizing flows can approximate complex probability distributions by applying a sequence of invertible transformations to a simpler base distribution, such as a multivariate Gaussian. If the base distribution has a joint probability density function (PDF) $p\\left(\\textbf{x}\\right)$, the sequence for invertible transformations $f_1, f_2, ... f_k$ is applied to the original variable $\\textbf{x}$ such that\n",
    "\n",
    "$$\\textbf{z} = f_k\\left(f_{k-1}\\left(...f_1\\left(\\textbf{x}\\right)\\right)\\right) = g\\left(\\textbf{x}\\right), $$\n",
    "\n",
    "where $\\textbf{z}$ is the transformed variable and $g\\left(\\textbf{x}\\right)$ is the composition of all other transformations $f_i$. This flexible framework allows for the learning of complex probability distributions that might not be obtainable by other methods.\n",
    "\n",
    "Additionally, we can see that one of the benefits of normalizing flows is efficient sampling of complex distributions. Samples can be easily calculated for the base distribution, such as a Gaussian, which then through the normalizing flow transform $g\\left(\\textbf{x}\\right)$ provides samples from the learned distribution.\n",
    "\n",
    "The transformed joint PDF $p\\left(\\textbf{z}\\right)$ can be easily calculated from the trained normalizing flow. The normalization condition for a PDF dictates that, for both $p\\left(\\textbf{x}\\right)$ and $p\\left(\\textbf{z}\\right)$,\n",
    "\n",
    "$$\\int ... \\int d x_1 ... d x_N \\; p\\left(\\textbf{x}\\right) = 1, \\tag{1}$$\n",
    "\n",
    "$$\\int ... \\int d z_1 ... d z_N \\; p\\left(\\textbf{z}\\right) = 1. $$\n",
    "\n",
    "Because $\\textbf{z} = g\\left(\\textbf{x}\\right)$, the second integral can be expressed in terms of $x$ as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    1 &= \\int ... \\int d z_1 ... d z_N \\; p\\left(\\textbf{z}\\right) \\\\\n",
    "    &= \\int ... \\int d x_1 ... d x_N \\left| \\text{det} \\frac{d \\textbf{z}}{d \\textbf{x}} \\right| p\\left(g\\left(x\\right)\\right),\n",
    "\\end{align}\\tag{2}\n",
    "$$\n",
    "\n",
    "where $\\frac{d \\textbf{z}}{d \\textbf{x}}$ is the matrix of first derivatives whose determinant is the *Jacobian* of the transformation function $g$. With the correct assumptions about the integration domain, comparison of Equations 1 and 2 yield\n",
    "\n",
    "$$p\\left(\\textbf{z}\\right) = p\\left(\\textbf{x}\\right) \\left| \\text{det} \\frac{d \\textbf{x}}{d \\textbf{z}} \\right|, $$\n",
    "\n",
    "which can be easily calculated from the trained normalizing flow using automatic differentiation. See these resources [[1]](https://en.wikibooks.org/wiki/Probability/Transformation_of_Random_Variables) [[2]](https://stats.libretexts.org/Bookshelves/Probability_Theory/Probability_Mathematical_Statistics_and_Stochastic_Processes_(Siegrist)/03%3A_Distributions/3.07%3A_Transformations_of_Random_Variables) [[3]](https://en.wikipedia.org/wiki/Probability_density_function)\n",
    "for more details about the transformation of random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial\n",
    "\n",
    "This tutorial will demonstrate the use of the `normflows` Python package (see the `normalizing-flows` [Github repository](https://github.com/VincentStimper/normalizing-flows)) for a simple example using a normalizing flow to approximate an unknown target distribution. An IPython notebook containing the full code in this tutorial can be found [here](https://github.com/jayspendlove/blog/tree/main/simple-normalizing-flow). For this example, the 2D target distribution $p(a,b)$ will consist of a random variable $a$ sampled from a uniform distribution, and another random variable $b$ sampled from a Normal distribution with mean and variance $a$. \n",
    "\n",
    "$$ a \\sim \\text{Uniform}\\left[1,2\\right] $$\n",
    "$$ b \\sim \\text{Normal}\\left(\\mu=a, \\sigma^2=a\\right). $$\n",
    "\n",
    "Stated differently, we want the normalizing flow approximation $NF\\left(a, b\\right) \\approx p\\left(a,b\\right)$. It is assumed that samples from the target distribution $p\\left(a,b\\right)$ can be easily obtained. \n",
    "\n",
    "This tutorial will begin by visualizing the target distribution $p\\left(a,b\\right)$, followed by construction and training of the normalizing flow on samples from $p\\left(a,b\\right)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import normflows as nf\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT INCLUDE IN BLOG POST\n",
    "# NF Cuda setup-- move model on GPU if available\n",
    "enable_cuda = False # not on my laptop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and enable_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the target distribution\n",
    "\n",
    "To help us get an idea of what this target distribution $p\\left(a,b\\right)$ looks like, we will visualize it in a couple different ways. First, we'll draw $10000$ samples of pairs $\\left(a,b\\right)$ from the distributions defined in Equations 4 and 5 and visualize the resulting 2D scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "sampled_a_vals = np.random.uniform(1, 2, N) # Uniform distribution\n",
    "sampled_b_vals = np.random.normal(sampled_a_vals, np.sqrt(sampled_a_vals)) # Normal distribution with mean a and variance a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT INCLUDE IN BLOG POST\n",
    "alim = (0.95, 2.05)\n",
    "blim = (-3, 7)\n",
    "\n",
    "# If I can delete a cell and not the picture output, I will for the plotting code\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(sampled_a_vals, sampled_b_vals, alpha=0.15)\n",
    "# For the b distribution, plot mean and STD's\n",
    "a_mean_var = np.linspace(1,2,100)\n",
    "plt.plot(a_mean_var, a_mean_var, 'r', label=\"Mean\")\n",
    "colors = ['b', 'lime', 'hotpink']\n",
    "for i,n_std in enumerate([1, 2, 3]):\n",
    "    plt.plot(a_mean_var, a_mean_var + n_std*np.sqrt(a_mean_var), c=colors[i], linestyle='--', label=f\"{n_std} STD\")\n",
    "    plt.plot(a_mean_var, a_mean_var - n_std*np.sqrt(a_mean_var), c=colors[i], linestyle='--')\n",
    "plt.xlabel(\"a\")\n",
    "plt.ylabel(\"b\")\n",
    "plt.xlim(*alim)\n",
    "plt.ylim(*blim)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that as expected, both the mean and the standard deviation of the $b$ distribution increase with increasing $a$. Note that the sampled points appear to have the highest density for low $a$. We can see the same thing by calculating the expected probability distribution $p\\left(a,b\\right)$ analytically, using the law of total probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_a(a):\n",
    "    \"\"\"Probability density function of a ~ Uniform[1,2]. Trivial function in this case, but included for consistency.\"\"\"\n",
    "    return np.where((a >= 1) & (a <= 2), 1, 0)\n",
    "\n",
    "def p_b_given_a(b, a):\n",
    "    \"\"\"Probability density function of b ~ Normal(a, a), p(b|a). \n",
    "    \n",
    "    Args:\n",
    "        b (np.ndarray): Values of b.\n",
    "        a (np.ndarray): Values of a, mean and variance of b.\n",
    "    \"\"\"\n",
    "    return (1 / (np.sqrt(2 * np.pi * a))) * np.exp(-0.5 * ((b - a)**2 / a))\n",
    "\n",
    "def p_a_b(ab):\n",
    "    \"\"\"Joint probability density p(a, b).\n",
    "    \n",
    "    Args:\n",
    "        ab (np.ndarray): 2D array where each row is a tuple (a, b).\n",
    "    \"\"\"\n",
    "    a = ab[:, 0]\n",
    "    b = ab[:, 1]\n",
    "    return p_a(a) * p_b_given_a(b, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT INCLUDE IN BLOG POST\n",
    "grid_size = [1000, 1000]\n",
    "a_vals = np.linspace(*alim, grid_size[0])\n",
    "b_vals = np.linspace(*blim, grid_size[1])\n",
    "aa, bb = np.meshgrid(a_vals, b_vals, indexing='ij')\n",
    "zz = np.stack([aa, bb], axis=2).reshape(-1, 2)\n",
    "\n",
    "prob = p_a_b(zz).reshape(*grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT INCLUDE IN BLOG POST\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pcolormesh(aa, bb, prob, cmap='coolwarm')\n",
    "plt.colorbar(label='Probability Density')\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "\n",
    "# mark mean and std like on other plot\n",
    "a_mean_var = np.linspace(1,2,100)\n",
    "plt.plot(a_mean_var, a_mean_var, 'r', label=\"Mean\")\n",
    "colors = ['b', 'lime', 'hotpink']\n",
    "for i,n_std in enumerate([1, 2, 3]):\n",
    "    plt.plot(a_mean_var, a_mean_var + n_std*np.sqrt(a_mean_var), c=colors[i], linestyle='--', label=f\"{n_std} STD\")\n",
    "    plt.plot(a_mean_var, a_mean_var - n_std*np.sqrt(a_mean_var), c=colors[i], linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to visualizing the full 2D distribution, we can look at just the $a$ and $b$ target distributions by marginalizing out the other variable. Because $a$ does not depend on $b$, the marginalized distribution for $a$ is just the $p\\left(a\\right) \\sim \\text{Uniform}\\left[1,2\\right]$. The marginal $b$ distribution, however, is more challenging to obtain because it depends on $a$ in a non-trivial way. If you think about it, as $a$ increases from $1$ to $2$, the distribution for $b$ will drift and spread out, meaning that the total marginal distribution of $b$ is a combination of all these different Normal distributions for different values of $a$. The marginal distribution $p\\left(b\\right)$ can be calculated by\n",
    "\n",
    "$$\n",
    "p(b) = \\int_1^2 da \\frac{1}{\\sqrt{2 \\pi a}} \\text{exp}\\left(\\frac{\\left(b-a\\right)^2}{2a}\\right).\n",
    "$$\n",
    "\n",
    "This integral is challenging to solve analytically, but can be calculated straightforwardly via numerical integration. Below is shown the marginal distributions for $a$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT INCLUDE IN BLOG POST\n",
    "def marginalize_p_b(p_a_b, a_vals, b_val):\n",
    "    \"\"\"\n",
    "    Calculate the marginal probability p(b) by integrating over the joint probability distribution p(a, b).\n",
    "\n",
    "    Args:\n",
    "        p_a_b (function): Joint probability distribution function p(a, b).\n",
    "        a_vals (numpy.ndarray): Array of a values to integrate over.\n",
    "        b_val (float): The value of b for which to calculate the marginal probability p(b).\n",
    "\n",
    "    Returns:\n",
    "        float: The marginal probability p(b).\n",
    "    \"\"\"\n",
    "    # Calculate the joint probability p(a, b) for each a in a_vals\n",
    "    p_a_b_vals = p_a_b(np.column_stack((a_vals, np.full_like(a_vals, b_val))))\n",
    "    \n",
    "    # Use the trapezoidal rule to integrate over a_vals\n",
    "    p_b = np.trapezoid(p_a_b_vals, a_vals)\n",
    "    \n",
    "    return p_b\n",
    "\n",
    "prob_b = np.array([marginalize_p_b(p_a_b, a_vals, b_val) for b_val in b_vals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT INCLUDE IN BLOG POST\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot a Target Distribution\n",
    "axs[0].hist(sampled_a_vals, bins=50, alpha=0.5, density=True)\n",
    "# axs[0].plot([0.9, 0.999, 1.0, 2.0, 2.001, 2.1], [0, 0, 1, 1, 0, 0], 'r')\n",
    "axs[0].plot(a_vals, p_a(a_vals), 'r')\n",
    "axs[0].set_xlim(*alim)\n",
    "axs[0].set_xlabel(\"a\")\n",
    "axs[0].set_ylabel(\"Probability Density\")\n",
    "axs[0].set_title(\"Marginalized a Target Distribution\")\n",
    "\n",
    "# Plot Marginalized b Target Distribution\n",
    "axs[1].plot(b_vals, prob_b, c=\"r\")\n",
    "axs[1].hist(sampled_b_vals, bins=50, alpha=0.4, density=True, color=\"tab:blue\")\n",
    "axs[1].set_xlim(*blim)\n",
    "axs[1].set_xlabel(\"b\")\n",
    "axs[1].set_ylabel(\"Probability Density\")\n",
    "axs[1].set_title(\"Marginalized b Target Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the normalizing flow\n",
    "\n",
    "Now that we understand our target distributions, we can begin to set up our normalizing flow to approximate $p(a,b)$.\n",
    "\n",
    "Here, we use the `normflows` Python package which utilizes Pytorch. Setting up a normalizing flow includes deciding on the number and type of flow layers and the base distribution. The base distribution is the initial choice of probability density, which the normalizing flow will transform over the course of training. These choices are problem specific. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NF architecture\n",
    "torch.manual_seed(0)\n",
    "K = 16 # number of repeated blocks\n",
    "latent_size = 2 # num input channels\n",
    "flows = []\n",
    "for i in range(K):\n",
    "    param_map = nf.nets.MLP([1, 64, 64, 2], init_zeros=True)\n",
    "    flows += [nf.flows.AffineCouplingBlock(param_map)]\n",
    "    flows += [nf.flows.LULinearPermute(latent_size)]\n",
    "\n",
    "base = nf.distributions.DiagGaussian(2, trainable=False) # Base distribution\n",
    "model = nf.NormalizingFlow(q0=base, flows=flows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the normalizing flow\n",
    "\n",
    "The next step is to train the normalizing flow on samples of $(a,b)$ from the true distributions. Here the loss function is chosen to be the negative log likelihood. Using the Adam optimizer, the normalizing flow is trained for 1000 epochs, with 512 samples of the true distribution per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT INCLUDE IN BLOG POST\n",
    "model = model.to(device)\n",
    "\n",
    "def sample_ab(num_samples, device):\n",
    "    '''Sample from a and b distributions'''\n",
    "    a = np.random.uniform(1, 2, num_samples)\n",
    "    b = np.array([np.random.normal(a_val, np.sqrt(a_val)) for a_val in a])\n",
    "    x_np = np.stack([a, b], axis=1)\n",
    "    x = torch.tensor(x_np).float().to(device)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT INCLUDE IN BLOG POST\n",
    "# Define before for saving the necessary information\n",
    "epochs = 1000\n",
    "loss_hist = np.zeros(epochs) # Store loss values\n",
    "\n",
    "# Plotting parameters for 2D probability density grids used in tutorial\n",
    "grid_size = (100, 120)\n",
    "a_bounds = [-0.5, 2.1]\n",
    "b_bounds = [-3, 6]\n",
    "a_vals = torch.linspace(*a_bounds, grid_size[0])\n",
    "b_vals = torch.linspace(*b_bounds, grid_size[1])\n",
    "aa, bb = torch.meshgrid(a_vals, b_vals, indexing='ij')\n",
    "zz = torch.cat([aa.unsqueeze(2), bb.unsqueeze(2)], 2).view(-1, 2)\n",
    "zz_np = zz.detach().numpy()\n",
    "zz = zz.to(device)\n",
    "vmax = 0.4\n",
    "\n",
    "# Save probability density for base distribution\n",
    "base_prob = base.log_prob(zz).exp().to('cpu').view(*aa.shape)\n",
    "\n",
    "# Save target probability density\n",
    "prob_target = p_a_b(zz.detach().numpy())\n",
    "prob_target[np.isnan(prob_target)] = 0 # set NaNs to 0\n",
    "prob_target = prob_target.reshape(*grid_size)\n",
    "\n",
    "# iterations to keep\n",
    "# iter_early = np.arange(1, 11, 2)\n",
    "n_samp = 5\n",
    "end = 20\n",
    "start = 1\n",
    "iter_early = np.arange(2, 21, (end-start)//(n_samp+1))\n",
    "print(iter_early)\n",
    "# iter_early = np.arange(1, end + 1, 2)\n",
    "show_iter = 200\n",
    "iter_later = np.arange(show_iter-1, epochs, show_iter)\n",
    "iterations = np.hstack((iter_early, iter_later)) #one less than \"human readable\" iteration number\n",
    "prob_grid = np.zeros((len(iterations), *grid_size)) # store probability grids\n",
    "print(iterations)\n",
    "\n",
    "def save_info(i, model, loss):\n",
    "    loss_hist[i] = loss.to('cpu').data.numpy()\n",
    "    if i in iterations:\n",
    "        model.eval() #change to evaluation mode\n",
    "        log_prob = model.log_prob(zz)\n",
    "        model.train() #change back to training mode\n",
    "        prob = torch.exp(log_prob.to('cpu').view(*aa.shape))\n",
    "        prob[torch.isnan(prob)] = 0 # set NaNs to 0\n",
    "        prob_grid[np.where(iterations == i)[0][0],:,:] = prob.data.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NF\n",
    "epochs = 1000\n",
    "num_samples = 2 ** 9 # 512 samples per iteration\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "for it in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    x = sample_ab(num_samples, device) # Get training samples\n",
    "    loss = -1*model.log_prob(x).mean() # Compute loss\n",
    "    # Do backprop and optimizer step\n",
    "    if ~(torch.isnan(loss) | torch.isinf(loss)): # Check for NaNs or infs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    save_info(it, model, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following training, we can see that in a small number of iterations the normalizing flow learned distribution was able to effectively capture the primary features of the target distribution. The error plot on the right shows where the true and learned distributions disagree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT INCLUDE IN BLOG POST\n",
    "# Plot base distribution, learned distribution, true distribution, and error side by side\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 7))\n",
    "\n",
    "# Plot base distribution\n",
    "axs[0].pcolormesh(aa, bb, base_prob.data.numpy(), cmap='coolwarm', vmin=0, vmax=vmax)\n",
    "axs[0].set_aspect('equal', 'box')\n",
    "axs[0].set_title(\"Base Distribution\")\n",
    "\n",
    "# Plot learned distribution\n",
    "model.eval()\n",
    "log_prob = model.log_prob(zz).to('cpu').view(*aa.shape)\n",
    "model.train()\n",
    "prob_nf = torch.exp(log_prob)\n",
    "prob_nf[torch.isnan(prob_nf)] = 0\n",
    "prob_nf = prob_nf.data.numpy()\n",
    "\n",
    "axs[1].pcolormesh(aa, bb, prob_nf, cmap='coolwarm', vmin=0, vmax=vmax)\n",
    "axs[1].set_aspect('equal', 'box')\n",
    "axs[1].set_title(\"Learned Distribution\")\n",
    "\n",
    "# Plot true distribution\n",
    "axs[2].pcolormesh(aa, bb, prob_target, cmap='coolwarm', vmin=0, vmax=vmax)\n",
    "axs[2].set_aspect('equal', 'box')\n",
    "axs[2].set_title(\"True Distribution\")\n",
    "\n",
    "# Plot error\n",
    "error = np.abs(prob_nf - prob_target)\n",
    "axs[3].pcolormesh(aa, bb, error, cmap='gray_r')\n",
    "axs[3].set_aspect('equal', 'box')\n",
    "axs[3].set_title(\"Error (|Learned - True|)\")\n",
    "\n",
    "# Add a shared colorbar for the first three plots\n",
    "cbar = fig.colorbar(axs[0].collections[0], ax=axs[:3], orientation='vertical', fraction=0.02, pad=0.04, aspect=70)\n",
    "cbar.set_label('Probability Density')\n",
    "\n",
    "# Add a separate colorbar for the error plot\n",
    "cbar_error = fig.colorbar(axs[3].collections[0], ax=axs[3], orientation='vertical', fraction=0.02, pad=0.04, aspect=70)\n",
    "cbar_error.set_label('Error')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of how quickly the normalizing flow converged to this approximation, we can look at the training loss over epochs. We see that the loss for the normalizing flow solution has plateaued by the end of training. In fact, even if we train for 10x as long the learned distribution does not improve significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(loss_hist, label='loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at probability densities at intermediate epochs, we can see that the approximate normalizing flow solution achieves reasonable accuracy early in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T INCLUDE CELL IN BLOG POST\n",
    "fig, axs = plt.subplots(1, len(iter_later) + 2, figsize=(18, 6),sharey=True)\n",
    "\n",
    "# Plot base distribution\n",
    "axs[0].pcolormesh(aa, bb, base_prob, cmap='coolwarm', vmin=0, vmax=vmax)\n",
    "axs[0].set_aspect('equal', 'box')\n",
    "axs[0].set_title(\"Base Distribution\")\n",
    "axs[0].set_xlabel(\"a\")\n",
    "axs[0].set_ylabel(\"b\")\n",
    "\n",
    "# Plot the first 5 saved densities in prob_grid\n",
    "for plot_i, iter in enumerate(iter_later):\n",
    "    iter_i = np.where(iterations == iter)[0][0]\n",
    "    axs[plot_i + 1].pcolormesh(aa, bb, prob_grid[iter_i], cmap='coolwarm', vmin=0, vmax=vmax)\n",
    "    axs[plot_i + 1].set_aspect('equal', 'box')\n",
    "    axs[plot_i + 1].set_title(f\"Epoch {iter + 1}\")\n",
    "    axs[plot_i + 1].set_xlabel(\"a\")\n",
    "\n",
    "# Plot the target probability density\n",
    "axs[-1].pcolormesh(aa, bb, prob_target, cmap='coolwarm', vmin=0, vmax=vmax)\n",
    "axs[-1].set_aspect('equal', 'box')\n",
    "axs[-1].set_title(\"Target Distribution\")\n",
    "axs[-1].set_xlabel(\"a\")\n",
    "\n",
    "# Add a shared colorbar\n",
    "cbar = fig.colorbar(axs[0].collections[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)#, fraction=0.02, pad=0.04, aspect=70)\n",
    "cbar.set_label('Probability Density')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the loss function, we see that the largest changes happen at the very beginning of training. In the subplots below, by looking at some of the first few epochs we can see the probability density migrating from the original Gaussian centered at $(0,0)$ towards the region with the target density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T INCLUDE CELL IN BLOG POST\n",
    "fig, axs = plt.subplots(1, len(iter_early) + 2, figsize=(18, 6),sharey=True)\n",
    "\n",
    "# Plot base distribution\n",
    "axs[0].pcolormesh(aa, bb, base_prob, cmap='coolwarm', vmin=0, vmax=vmax)\n",
    "axs[0].set_aspect('equal', 'box')\n",
    "axs[0].set_title(\"Base Distribution\")\n",
    "axs[0].set_xlabel(\"a\")\n",
    "axs[0].set_ylabel(\"b\")\n",
    "\n",
    "# Plot the first 5 saved densities in prob_grid\n",
    "for i, iter in enumerate(iter_early):\n",
    "    axs[i + 1].pcolormesh(aa, bb, prob_grid[i], cmap='coolwarm', vmin=0, vmax=vmax)\n",
    "    axs[i + 1].set_aspect('equal', 'box')\n",
    "    axs[i + 1].set_title(f\"Epoch {iter + 1}\")\n",
    "    axs[i + 1].set_xlabel(\"a\")\n",
    "\n",
    "# Plot the target probability density\n",
    "axs[-1].pcolormesh(aa, bb, prob_target, cmap='coolwarm', vmin=0, vmax=vmax)\n",
    "axs[-1].set_aspect('equal', 'box')\n",
    "axs[-1].set_title(\"Target Distribution\")\n",
    "axs[-1].set_xlabel(\"a\")\n",
    "\n",
    "# Add a shared colorbar\n",
    "cbar = fig.colorbar(axs[0].collections[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)#, fraction=0.02, pad=0.04, aspect=70)\n",
    "cbar.set_label('Probability Density')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another perspective of the normalizing flow solution, we can look at the marginalized probability densities for $a$ and $b$ and compare the true and approximate solutions, in red and black respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT INCLUDE IN BLOG POST\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot a Target Distribution\n",
    "prob = torch.exp(model.log_prob(zz).view(*aa.shape))\n",
    "prob[torch.isnan(prob)] = 0  # Fix NaNs\n",
    "prob_marg = prob.sum(dim=1).detach().numpy()\n",
    "da = (a_vals[1] - a_vals[0]).numpy()\n",
    "prob_marg_norm = prob_marg / (prob_marg.sum()*da)\n",
    "axs[0].plot(a_vals, prob_marg_norm, c=\"k\", label=\"NF\", linestyle=\"dashed\")#, linewidth=3)\n",
    "axs[0].plot(a_vals, p_a(a_vals), 'r', label=\"True\")\n",
    "axs[0].set_xlabel(\"a\")\n",
    "axs[0].set_ylabel(\"Probability Density\")\n",
    "axs[0].set_title(\"Marginalized a distribution\")\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot Marginalized b Target Distribution\n",
    "# NF estimate\n",
    "prob = torch.exp(model.log_prob(zz).view(*aa.shape))\n",
    "prob[torch.isnan(prob)] = 0  # Fix NaNs\n",
    "prob_marg = prob.sum(dim=0).detach().numpy()\n",
    "db = (b_vals[1] - b_vals[0]).numpy()\n",
    "prob_marg_norm = prob_marg / (prob_marg.sum()*db)\n",
    "axs[1].plot(b_vals, prob_marg_norm, label=\"NF\", c=\"k\", linestyle=\"dashed\")#, linewidth=3)\n",
    "# True marginalized b distribution\n",
    "a_vals_int = np.linspace(1, 2, 1000)\n",
    "prob_b = np.array([marginalize_p_b(p_a_b, a_vals_int, b_val) for b_val in b_vals.numpy()])\n",
    "\n",
    "axs[1].plot(b_vals, prob_b, c=\"r\", label=\"True\")\n",
    "axs[1].set_xlabel(\"b\")\n",
    "axs[1].set_ylabel(\"Probability Density\")\n",
    "axs[1].set_title(\"Marginalized b Distribution\")\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the trained normalizing flow\n",
    "\n",
    "Once the normalizing flow has been trained, sampling from it is trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() #Set model to evaluation mode\n",
    "samples, log_prob = model.sample(num_samples=1000) #Sample from normalizing flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT INCLUDE IN BLOG POST\n",
    "# Extract a and b from the samples\n",
    "a_samples = samples[:, 0].detach().numpy()\n",
    "b_samples = samples[:, 1].detach().numpy()\n",
    "\n",
    "# Plot the sampled values\n",
    "plt.figure()\n",
    "plt.scatter(a_samples, b_samples, alpha=0.5)\n",
    "plt.xlabel(\"a\")\n",
    "plt.ylabel(\"b\")\n",
    "plt.title(\"Samples from the trained Normalizing Flow\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "Some of the code in this tutorial was adapted from example scripts in the `normalizing-flows` repository: https://github.com/VincentStimper/normalizing-flows\n",
    "\n",
    "Stimper et al., (2023). normflows: A PyTorch Package for Normalizing Flows. Journal of Open Source Software, 8(86), 5361, https://doi.org/10.21105/joss.05361\n",
    "\n",
    "## Author\n",
    "Jay Spendlove\n",
    "\n",
    "PhD student, Arizona State University\n",
    "\n",
    "jcspendl@asu.edu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "norm_flows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
